# AI Agents Evaluation Sample

This sample demonstrates evaluation techniques for AI agents using Python and Jupyter notebooks.

## Purpose
- Evaluate AI agent performance across different frameworks
- Measure tool calling accuracy and effectiveness
- Assess response quality and consistency
- Compare Azure AI Agent Service vs Semantic Kernel Agents implementations

## Features
- **Performance Metrics**: Response time, success rates, error handling
- **Tool Calling Evaluation**: Accuracy of function calls, parameter handling
- **Quality Assessment**: Response relevance, coherence, and helpfulness
- **Comparative Analysis**: Side-by-side framework comparison
- **Visualization**: Charts and graphs for evaluation results

## Setup
1. Ensure Python environment is configured with required packages
2. Configure AI Foundry connection in `.env` file
3. Run the notebook cells sequentially
4. Review evaluation results and visualizations

## Dependencies
- azure-ai-evaluation
- azure-identity
- python-dotenv
- pandas
- matplotlib
- seaborn

## Usage
```bash
cd samples/18-ai-agents-evaluation
# Open in VS Code or Jupyter
code ai-agents-evaluation.ipynb
```

## Related Samples
- Sample 16: Azure AI Agent Service (evaluation target)
- Sample 17: Semantic Kernel Agents (evaluation target)
- Sample 13: AI Foundry Evaluations (evaluation framework reference)
