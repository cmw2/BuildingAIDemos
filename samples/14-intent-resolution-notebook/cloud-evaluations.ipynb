{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0c0ebe",
   "metadata": {},
   "source": [
    "# ðŸ‹ï¸â€â™€ï¸ Health & Fitness Evaluations with Azure AI Foundry ðŸ‹ï¸â€â™‚ï¸\n",
    "\n",
    "This notebook demonstrates how to **evaluate** a Generative AI model (or application) using the **Azure AI Foundry** ecosystem. We'll highlight three key Python SDKs:\n",
    "1. **`azure-ai-projects`** (`AIProjectClient`): manage & orchestrate evaluations in the cloud.\n",
    "2. **`azure-ai-inference`**: perform model inference (optional but helpful if generating data for evaluation).\n",
    "3. **`azure-ai-evaluation`**: run automated metrics for LLM output quality & safety.\n",
    "\n",
    "We'll create or use some synthetic \"health & fitness\" Q&A data, then measure how well your model is answering. We'll do both **local** evaluation and **cloud** evaluation (on an Azure AI Foundry project).\n",
    "\n",
    "> **Disclaimer**: This covers a hypothetical health & fitness scenario. **No real medical advice** is provided. Always consult professionals.\n",
    "\n",
    "## Notebook Contents\n",
    "1. [Setup & Imports](#1-Setup-and-Imports)\n",
    "2. [Local Evaluation Examples](#3-Local-Evaluation)\n",
    "3. [Cloud Evaluation with `AIProjectClient`](#4-Cloud-Evaluation)\n",
    "4. [Extra Topics](#5-Extra-Topics)\n",
    "   - [Risk & Safety Evaluators](#5.1-Risk-and-Safety)\n",
    "   - [More Quality Evaluators](#5.2-Quality)\n",
    "   - [Custom Evaluators](#5.3-Custom)\n",
    "   - [Simulators & Adversarial Data](#5.4-Simulators)\n",
    "5. [Conclusion](#6-Conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfadf84",
   "metadata": {
    "id": "1-Setup-and-Imports"
   },
   "source": [
    "## 1. Setup and Imports\n",
    "We'll install necessary libraries, import them, and define some synthetic data. \n",
    "\n",
    "### Dependencies\n",
    "- `azure-ai-projects` for orchestrating evaluations in your Azure AI Foundry Project.\n",
    "- `azure-ai-evaluation` for built-in or custom metrics (like Relevance, Groundedness, F1Score, etc.).\n",
    "- `azure-ai-inference` (optional) if you'd like to generate completions to produce data to evaluate.\n",
    "- `azure-identity` (for Azure authentication via `DefaultAzureCredential`).\n",
    "\n",
    "### Synthetic Data\n",
    "We'll create a small JSONL with *health & fitness* Q&A pairs, including `query`, `response`, `context`, and `ground_truth`. This simulates a scenario where we have user questions, the model's answers, plus a reference ground truth.\n",
    "\n",
    "You can adapt this approach to any domain: e.g., finance, e-commerce, etc.\n",
    "\n",
    "<img src=\"./seq-diagrams/2-evals.png\" alt=\"Evaluation Flow\" width=\"30%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b889daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# If you need to install these, uncomment:\n",
    "# !pip install azure-ai-projects azure-ai-evaluation azure-ai-inference azure-identity\n",
    "# !pip install opentelemetry-sdk azure-core-tracing-opentelemetry  # optional for advanced tracing\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# We'll create a synthetic dataset in JSON Lines format\n",
    "synthetic_eval_data = [\n",
    "    {\n",
    "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
    "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
    "        \"response\": \"You can just go for 10 push-ups total.\",\n",
    "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
    "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
    "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
    "        \"ground_truth\": \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the capital of France?\",\n",
    "        \"context\": \"France is in Europe. Paris is the capital.\",\n",
    "        \"response\": \"London.\",\n",
    "        \"ground_truth\": \"Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write them to a local JSONL file\n",
    "eval_data_path = Path(\"./health_fitness_eval_data.jsonl\")\n",
    "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in synthetic_eval_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"Sample evaluation data written to {eval_data_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5598",
   "metadata": {
    "id": "3-Local-Evaluation"
   },
   "source": [
    "# 3. Local Evaluation Examples\n",
    "\n",
    "We'll show how to run local, code-based evaluation on a JSONL dataset. We'll:\n",
    "1. **Load** the data.\n",
    "2. **Define** one or more evaluators. (e.g. `F1ScoreEvaluator`, `RelevanceEvaluator`, `GroundednessEvaluator`, or custom.)\n",
    "3. **Run** `evaluate(...)` to produce a dictionary of metrics.\n",
    "\n",
    "> We can also do multi-turn conversation data or add extra columns like `ground_truth` for advanced metrics.\n",
    "\n",
    "## Example 1: Combining F1Score, Relevance & Groundedness\n",
    "We'll combine:\n",
    "- `F1ScoreEvaluator` (NLP-based, compares `response` to `ground_truth`)\n",
    "- `RelevanceEvaluator` (AI-assisted, uses GPT to judge how well `response` addresses `query`)\n",
    "- `GroundednessEvaluator` (checks how well the response is anchored in the provided `context`)\n",
    "- A custom code-based evaluator that logs response length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f04f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:azure.monitor.opentelemetry.exporter.export._base:Non-retryable server side error: Operation returned an invalid status 'Bad Request'.\n",
      "[2025-07-27 23:16:03 -0400][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-27 23:16:03 -0400][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-27 23:16:03 -0400][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-27 23:16:03 -0400][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-07-27 23:16:03 -0400][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_groundedness_20250727_231555_335515, log path: C:\\Users\\chwestbr\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_groundedness_20250727_231555_335515\\logs.txt\n",
      "[2025-07-27 23:16:03 -0400][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_f1_score_20250727_231555_325884, log path: C:\\Users\\chwestbr\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_f1_score_20250727_231555_325884\\logs.txt\n",
      "[2025-07-27 23:16:03 -0400][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_relevance_20250727_231555_335515, log path: C:\\Users\\chwestbr\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_relevance_20250727_231555_335515\\logs.txt\n",
      "[2025-07-27 23:16:03 -0400][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_resp_len_20250727_231555_335515, log path: C:\\Users\\chwestbr\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_resp_len_20250727_231555_335515\\logs.txt\n",
      "ERROR:azure.monitor.opentelemetry.exporter.export._base:Non-retryable server side error: Operation returned an invalid status 'Bad Request'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 23:16:03 -0400   29348 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-07-27 23:16:03 -0400   29348 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-07-27 23:16:03 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 0.03 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_f1_score_20250727_231555_325884\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-07-27 23:15:55.367573-04:00\"\n",
      "Duration: \"0:00:09.242419\"\n",
      "Output path: \"C:\\Users\\chwestbr\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_f1_score_20250727_231555_325884\"\n",
      "\n",
      "2025-07-27 23:16:07 -0400   29348 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-07-27 23:16:07 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 4.4 seconds. Estimated time for incomplete lines: 8.8 seconds.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 4.6 seconds. Estimated time for incomplete lines: 9.2 seconds.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-2)-Process id(38020)-Line number(0) start execution.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-3)-Process id(18976)-Line number(1) start execution.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-4)-Process id(25540)-Line number(2) start execution.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 2.36 seconds. Estimated time for incomplete lines: 2.36 seconds.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 2.33 seconds. Estimated time for incomplete lines: 2.33 seconds.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 1.59 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 1.57 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-27 23:16:03 -0400   29348 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 4.6 seconds. Estimated time for incomplete lines: 9.2 seconds.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 2.36 seconds. Estimated time for incomplete lines: 2.36 seconds.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 1.59 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_groundedness_20250727_231555_335515\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-07-27 23:15:55.367573-04:00\"\n",
      "Duration: \"0:00:13.208404\"\n",
      "Output path: \"C:\\Users\\chwestbr\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_groundedness_20250727_231555_335515\"\n",
      "\n",
      "2025-07-27 23:16:03 -0400   29348 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-07-27 23:16:07 -0400   29348 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-07-27 23:16:07 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 4.4 seconds. Estimated time for incomplete lines: 8.8 seconds.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 2.33 seconds. Estimated time for incomplete lines: 2.33 seconds.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 1.57 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_relevance_20250727_231555_335515\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-07-27 23:15:55.367573-04:00\"\n",
      "Duration: \"0:00:13.212401\"\n",
      "Output path: \"C:\\Users\\chwestbr\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_relevance_20250727_231555_335515\"\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:azure.monitor.opentelemetry.exporter.export._base:Non-retryable server side error: Operation returned an invalid status 'Bad Request'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 23:16:12 -0400   29348 execution.bulk     WARNING  Process crashed while executing line 0.\n",
      "2025-07-27 23:16:12 -0400   29348 execution.bulk     ERROR    Line 0, Process 29348 failed with exception: Process crashed while executing line 0,\n",
      "2025-07-27 23:16:12 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-2)-Process id(38020)-Line number(0) failed.\n",
      "2025-07-27 23:16:12 -0400   29348 execution.bulk     WARNING  Process 38020 had been terminated.\n",
      "2025-07-27 23:16:13 -0400   29348 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-07-27 23:16:13 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 5.05 seconds. Estimated time for incomplete lines: 10.1 seconds.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     WARNING  Process crashed while executing line 2.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     WARNING  Process crashed while executing line 1.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     ERROR    Line 2, Process 29348 failed with exception: Process crashed while executing line 2,\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     ERROR    Line 1, Process 29348 failed with exception: Process crashed while executing line 1,\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-4)-Process id(25540)-Line number(2) failed.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     WARNING  Process 25540 had been terminated.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-3)-Process id(18976)-Line number(1) failed.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     WARNING  Process 18976 had been terminated.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 3.36 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     The thread monitoring the process [35828-SpawnProcess-5] will be terminated.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     The thread monitoring the process [37816-SpawnProcess-6] will be terminated.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     The thread monitoring the process [28304-SpawnProcess-7] will be terminated.\n",
      "2025-07-27 23:16:20 -0400   29348 execution.bulk     INFO     Process 35828 terminated.\n",
      "2025-07-27 23:16:25 -0400   29348 execution.bulk     INFO     Process 37816 terminated.\n",
      "2025-07-27 23:16:25 -0400   29348 execution.bulk     INFO     Process 28304 terminated.\n",
      "2025-07-27 23:16:25 -0400   29348 execution          ERROR    3/3 flow run failed, indexes: [0,1,2], exception of index 0: Process crashed while executing line 0,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-27 23:16:25 -0400][promptflow._sdk._orchestrator.run_submitter][WARNING] - 3 out of 3 runs failed in batch run.\n",
      " Please check out C:/Users/chwestbr/.promptflow/.runs/azure_ai_evaluation_evaluators_resp_len_20250727_231555_335515 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-27 23:16:03 -0400   29348 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-07-27 23:16:03 -0400   29348 execution.bulk     INFO     The timeout for the batch run is 3600 seconds.\n",
      "2025-07-27 23:16:03 -0400   29348 execution.bulk     INFO     Current system's available memory is 2041.9140625MB, memory consumption of current process is 317.91015625MB, estimated available worker count is 2041.9140625/317.91015625 = 6\n",
      "2025-07-27 23:16:03 -0400   29348 execution.bulk     INFO     Set process count to 3 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 3, 'estimated_worker_count_based_on_memory_usage': 6}.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-2)-Process id(38020)-Line number(0) start execution.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-3)-Process id(18976)-Line number(1) start execution.\n",
      "2025-07-27 23:16:08 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-4)-Process id(25540)-Line number(2) start execution.\n",
      "2025-07-27 23:16:12 -0400   29348 execution.bulk     WARNING  Process crashed while executing line 0.\n",
      "2025-07-27 23:16:12 -0400   29348 execution.bulk     ERROR    Line 0, Process 29348 failed with exception: Process crashed while executing line 0,\n",
      "2025-07-27 23:16:12 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-2)-Process id(38020)-Line number(0) failed.\n",
      "2025-07-27 23:16:12 -0400   29348 execution.bulk     WARNING  Process 38020 had been terminated.\n",
      "2025-07-27 23:16:13 -0400   29348 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-07-27 23:16:13 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 5.05 seconds. Estimated time for incomplete lines: 10.1 seconds.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     WARNING  Process crashed while executing line 2.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     WARNING  Process crashed while executing line 1.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     ERROR    Line 2, Process 29348 failed with exception: Process crashed while executing line 2,\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     ERROR    Line 1, Process 29348 failed with exception: Process crashed while executing line 1,\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-4)-Process id(25540)-Line number(2) failed.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     WARNING  Process 25540 had been terminated.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     INFO     Process name(SpawnProcess-3)-Process id(18976)-Line number(1) failed.\n",
      "2025-07-27 23:16:17 -0400   29348 execution.bulk     WARNING  Process 18976 had been terminated.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     Average execution time for completed lines: 3.36 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     The thread monitoring the process [35828-SpawnProcess-5] will be terminated.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     The thread monitoring the process [37816-SpawnProcess-6] will be terminated.\n",
      "2025-07-27 23:16:18 -0400   29348 execution.bulk     INFO     The thread monitoring the process [28304-SpawnProcess-7] will be terminated.\n",
      "2025-07-27 23:16:20 -0400   29348 execution.bulk     INFO     Process 35828 terminated.\n",
      "2025-07-27 23:16:25 -0400   29348 execution.bulk     INFO     Process 37816 terminated.\n",
      "2025-07-27 23:16:25 -0400   29348 execution.bulk     INFO     Process 28304 terminated.\n",
      "2025-07-27 23:16:25 -0400   29348 execution          ERROR    3/3 flow run failed, indexes: [0,1,2], exception of index 0: Process crashed while executing line 0,\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_resp_len_20250727_231555_335515\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-07-27 23:15:55.367573-04:00\"\n",
      "Duration: \"0:00:30.328720\"\n",
      "Output path: \"C:\\Users\\chwestbr\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_resp_len_20250727_231555_335515\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"f1_score\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:09.242419\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\chwestbr\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_f1_score_20250727_231555_325884\"\n",
      "    },\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:13.212401\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\chwestbr\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_relevance_20250727_231555_335515\"\n",
      "    },\n",
      "    \"groundedness\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:13.208404\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\chwestbr\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_groundedness_20250727_231555_335515\"\n",
      "    },\n",
      "    \"resp_len\": {\n",
      "        \"status\": \"Completed with Errors\",\n",
      "        \"duration\": \"0:00:30.328720\",\n",
      "        \"completed_lines\": 0,\n",
      "        \"failed_lines\": 3,\n",
      "        \"log_path\": \"C:\\\\Users\\\\chwestbr\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_resp_len_20250727_231555_335515\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "Local evaluation result =>\n",
      "{'rows': [{'inputs.query': 'How can I start a beginner workout routine at home?', 'inputs.context': 'Workout routines can include push-ups, bodyweight squats, lunges, and planks.', 'inputs.response': 'You can just go for 10 push-ups total.', 'inputs.ground_truth': 'At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.', 'outputs.f1_score.f1_score': 0.30000000000000004, 'outputs.f1_score.f1_result': 'fail', 'outputs.f1_score.f1_threshold': 0.5, 'outputs.relevance.relevance': 3, 'outputs.relevance.gpt_relevance': 3, 'outputs.relevance.relevance_reason': 'The RESPONSE addresses the QUERY but omits essential details necessary for a full understanding of how to start a beginner workout routine at home.', 'outputs.relevance.relevance_result': 'pass', 'outputs.relevance.relevance_threshold': 3, 'outputs.groundedness.groundedness': 4, 'outputs.groundedness.gpt_groundedness': 4, 'outputs.groundedness.groundedness_reason': 'The RESPONSE is grounded in the CONTEXT but omits critical details about the other exercises listed, making it incomplete.', 'outputs.groundedness.groundedness_result': 'pass', 'outputs.groundedness.groundedness_threshold': 3}, {'inputs.query': 'Are diet sodas healthy for daily consumption?', 'inputs.context': 'Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.', 'inputs.response': 'Yes, diet sodas are 100% healthy.', 'inputs.ground_truth': \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\", 'outputs.f1_score.f1_score': 0.2608695652, 'outputs.f1_score.f1_result': 'fail', 'outputs.f1_score.f1_threshold': 0.5, 'outputs.relevance.relevance': 2, 'outputs.relevance.gpt_relevance': 2, 'outputs.relevance.relevance_reason': 'The RESPONSE attempts to answer the QUERY but provides incorrect information, making it an inaccurate response.', 'outputs.relevance.relevance_result': 'fail', 'outputs.relevance.relevance_threshold': 3, 'outputs.groundedness.groundedness': 2, 'outputs.groundedness.gpt_groundedness': 2, 'outputs.groundedness.groundedness_reason': 'The RESPONSE contradicts the CONTEXT by asserting that diet sodas are \"100% healthy,\" which is not supported by the CONTEXT\\'s mention of artificial sweeteners.', 'outputs.groundedness.groundedness_result': 'fail', 'outputs.groundedness.groundedness_threshold': 3}, {'inputs.query': \"What's the capital of France?\", 'inputs.context': 'France is in Europe. Paris is the capital.', 'inputs.response': 'London.', 'inputs.ground_truth': 'Paris.', 'outputs.f1_score.f1_score': 0.0, 'outputs.f1_score.f1_result': 'fail', 'outputs.f1_score.f1_threshold': 0.5, 'outputs.relevance.relevance': 2, 'outputs.relevance.gpt_relevance': 2, 'outputs.relevance.relevance_reason': 'The RESPONSE is incorrect because it provides the wrong capital for France, making it an incorrect attempt to answer the QUERY.', 'outputs.relevance.relevance_result': 'fail', 'outputs.relevance.relevance_threshold': 3, 'outputs.groundedness.groundedness': 1, 'outputs.groundedness.gpt_groundedness': 1, 'outputs.groundedness.groundedness_reason': 'The RESPONSE is completely ungrounded as it introduces unrelated information and does not adhere to the CONTEXT.', 'outputs.groundedness.groundedness_result': 'fail', 'outputs.groundedness.groundedness_threshold': 3}], 'metrics': {'f1_score.f1_score': 0.18695652173333333, 'f1_score.f1_threshold': 0.5, 'relevance.relevance': 2.3333333333333335, 'relevance.gpt_relevance': 2.3333333333333335, 'relevance.relevance_threshold': 3.0, 'groundedness.groundedness': 2.3333333333333335, 'groundedness.gpt_groundedness': 2.3333333333333335, 'groundedness.groundedness_threshold': 3.0, 'f1_score.binary_aggregate': 0.0, 'relevance.binary_aggregate': 0.33, 'groundedness.binary_aggregate': 0.33}, 'studio_url': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:azure.monitor.opentelemetry.exporter.export._base:Non-retryable server side error: Operation returned an invalid status 'Bad Request'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created AIProjectClient.\n",
      "âœ… Created AIProjectClient.\n",
      "âœ… Created AIProjectClient.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from azure.ai.evaluation import (\n",
    "    evaluate,\n",
    "    F1ScoreEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    GroundednessEvaluator\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# Our custom evaluator to measure response length.\n",
    "def response_length_eval(response, **kwargs):\n",
    "    return {\"resp_length\": len(response)}\n",
    "\n",
    "# We'll define a credential-based config for AI-assisted evaluators.\n",
    "# This uses DefaultAzureCredential instead of API keys.\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://dummy-endpoint.azure.com\"),\n",
    "    \"azure_ad_token_provider\": token_provider,\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4\"),\n",
    "    \"api_version\": os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2023-07-01-preview\"),\n",
    "}\n",
    "\n",
    "eval_data_path = Path(\"./health_fitness_eval_data.jsonl\")\n",
    "\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "rel_eval = RelevanceEvaluator(model_config=model_config)\n",
    "ground_eval = GroundednessEvaluator(model_config=model_config)\n",
    "\n",
    "# We'll run evaluate(...) with these evaluators.\n",
    "results = evaluate(\n",
    "    data=str(eval_data_path),\n",
    "    evaluators={\n",
    "        \"f1_score\": f1_eval,\n",
    "        \"relevance\": rel_eval,\n",
    "        \"groundedness\": ground_eval,\n",
    "        \"resp_len\": response_length_eval\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"f1_score\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\"\n",
    "            }\n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"resp_len\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Local evaluation result =>\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d525400",
   "metadata": {},
   "source": [
    "**Inspecting Local Results**\n",
    "\n",
    "The `evaluate(...)` call returns a dictionary with:\n",
    "- **`metrics`**: aggregated metrics across rows (like average F1, Relevance, or Groundedness)\n",
    "- **`rows`**: row-by-row results with inputs and evaluator outputs\n",
    "- **`traces`**: debugging info (if any)\n",
    "\n",
    "You can further analyze these results, store them in a database, or integrate them into your CI/CD pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b903ea",
   "metadata": {
    "id": "4-Cloud-Evaluation"
   },
   "source": [
    "# 4. Cloud Evaluation with `AIProjectClient`\n",
    "\n",
    "Sometimes, we want to:\n",
    "- Evaluate large or sensitive datasets in the cloud (scalability, governed access).\n",
    "- Keep track of evaluation results in an Azure AI Foundry project.\n",
    "- Optionally schedule recurring evaluations.\n",
    "\n",
    "We'll do that by:\n",
    "1. **Upload** the local JSONL to your Azure AI Foundry project.\n",
    "2. **Create** an `Evaluation` referencing built-in or custom evaluator definitions.\n",
    "3. **Poll** until the job is done (with retry logic for resilience).\n",
    "4. **Review** the results in the portal or via `project_client.evaluations.get(...)`.\n",
    "\n",
    "### Prerequisites\n",
    "- An Azure AI Foundry project with a valid **Connection String** (from your projectâ€™s Overview page).\n",
    "- An Azure OpenAI deployment (if using AI-assisted evaluators).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68d936ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Created AIProjectClient.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 2) Upload data for evaluation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m uploaded_data_id, _ = project_client.datasets.upload_file(name=\u001b[33m'\u001b[39m\u001b[33meval-data\u001b[39m\u001b[33m'\u001b[39m, version=\u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m, file_path=\u001b[38;5;28mstr\u001b[39m(eval_data_path))\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ… Uploaded JSONL to project. Data asset ID:\u001b[39m\u001b[33m\"\u001b[39m, uploaded_data_id)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 3) Prepare an Azure OpenAI connection for AI-assisted evaluators\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation, InputDataset, EvaluatorConfiguration, ConnectionType\n",
    ")\n",
    "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.exceptions import ServiceResponseError\n",
    "import time\n",
    "\n",
    "# 1) Connect to Azure AI Foundry project\n",
    "project_conn_str = os.environ.get(\"AI_FOUNDRY_PROJECT_CONNECTION_STRING\")\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=project_conn_str,\n",
    "    credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "# project_client = AIProjectClient.from_connection_string(\n",
    "#     credential=credential,\n",
    "#     conn_str=project_conn_str\n",
    "# )\n",
    "print(\"âœ… Created AIProjectClient.\")\n",
    "\n",
    "# 2) Upload data for evaluation\n",
    "uploaded_data_id, _ = project_client.datasets.upload_file(name='eval-data', version='1', file_path=str(eval_data_path))\n",
    "print(\"âœ… Uploaded JSONL to project. Data asset ID:\", uploaded_data_id)\n",
    "\n",
    "# 3) Prepare an Azure OpenAI connection for AI-assisted evaluators\n",
    "default_conn = project_client.connections.get_default(ConnectionType.AZURE_OPEN_AI)\n",
    "\n",
    "deployment_name = os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"gpt-4\")\n",
    "api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2023-07-01-preview\")\n",
    "\n",
    "# 4) Construct the evaluation object\n",
    "model_config = default_conn.to_evaluator_model_config(\n",
    "    deployment_name=deployment_name,\n",
    "    api_version=api_version\n",
    ")\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Health Fitness Remote Evaluation\",\n",
    "    description=\"Evaluating dataset for correctness.\",\n",
    "    data=InputDatasetaset(id=uploaded_data_id),\n",
    "    evaluators={\n",
    "        \"f1_score\": EvaluatorConfiguration(id=F1ScoreEvaluator.id),\n",
    "        \"relevance\": EvaluatorConfiguration(\n",
    "            id=RelevanceEvaluator.id,\n",
    "            init_params={\"model_config\": model_config}\n",
    "        ),\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=ViolenceEvaluator.id,\n",
    "            init_params={\"azure_ai_project\": project_client.scope}\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Helper: Create evaluation with retry logic\n",
    "def create_evaluation_with_retry(project_client, evaluation, max_retries=3, retry_delay=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = project_client.evaluations.create(evaluation=evaluation)\n",
    "            return result\n",
    "        except ServiceResponseError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            print(f\"âš ï¸ Attempt {attempt+1} failed: {str(e)}. Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "# 5) Create & track the evaluation using retry logic\n",
    "cloud_eval = create_evaluation_with_retry(project_client, evaluation)\n",
    "print(\"âœ… Created evaluation job. ID:\", cloud_eval.id)\n",
    "\n",
    "# 6) Poll or fetch final status\n",
    "fetched_eval = project_client.evaluations.get(cloud_eval.id)\n",
    "print(\"Current status:\", fetched_eval.status)\n",
    "if hasattr(fetched_eval, 'properties'):\n",
    "    link = fetched_eval.properties.get(\"AiStudioEvaluationUri\", \"\")\n",
    "    if link:\n",
    "        print(\"View details in Foundry:\", link)\n",
    "else:\n",
    "    print(\"No link found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091290cd",
   "metadata": {},
   "source": [
    "### Viewing Cloud Evaluation Results\n",
    "- Navigate to the **Evaluations** tab in your AI Foundry project to see your evaluation job.\n",
    "- Open the evaluation to view aggregated metrics and row-level details.\n",
    "- For AI-assisted or risk & safety evaluators, you'll see both average scores and detailed per-row results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e3a2c4",
   "metadata": {},
   "source": [
    "# 5. Extra Topics\n",
    "We'll do a quick overview of some advanced features:\n",
    "1. [Risk & Safety Evaluators](#5.1-Risk-and-Safety)\n",
    "2. [Additional Quality Evaluators](#5.2-Quality)\n",
    "3. [Custom Evaluators](#5.3-Custom)\n",
    "4. [Simulators & Adversarial Data](#5.4-Simulators)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7490e0eb",
   "metadata": {
    "id": "5.1-Risk-and-Safety"
   },
   "source": [
    "## 5.1 Risk & Safety Evaluators\n",
    "\n",
    "Azure AI Foundry includes built-in evaluators that detect content risks. Examples include:\n",
    "- **ViolenceEvaluator**: detects violent or harmful content.\n",
    "- **SexualEvaluator**: checks for explicit content.\n",
    "- **HateUnfairnessEvaluator**: flags hateful content.\n",
    "- **SelfHarmEvaluator**: detects self-harm related content.\n",
    "- **ProtectedMaterialEvaluator**: identifies copyrighted or protected content.\n",
    "\n",
    "These evaluators accept a `query` and `response` (and sometimes `context`) to provide severity labels and scores.\n",
    "\n",
    "For example:\n",
    "```python\n",
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "\n",
    "violence_eval = ViolenceEvaluator(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    azure_ai_project={\n",
    "        \"subscription_id\": \"...\",\n",
    "        \"resource_group_name\": \"...\",\n",
    "        \"project_name\": \"...\"\n",
    "    }\n",
    ")\n",
    "result = violence_eval(query=\"What is the capital of France?\", response=\"Paris\")\n",
    "print(result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a94f46",
   "metadata": {
    "id": "5.2-Quality"
   },
   "source": [
    "## 5.2 Additional Quality Evaluators\n",
    "Beyond `F1Score` and `Relevance`, there are many built-ins:\n",
    "- **GroundednessEvaluator**: Checks if the response is anchored in the provided context.\n",
    "- **CoherenceEvaluator**: Measures the logical flow of the response.\n",
    "- **FluencyEvaluator**: Assesses grammatical correctness.\n",
    "\n",
    "These metrics can help you fine-tune your modelâ€™s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f718e",
   "metadata": {
    "id": "5.3-Custom"
   },
   "source": [
    "## 5.3 Custom Evaluators\n",
    "You can build your own evaluators. For instance, a simple evaluator that measures the length of a response:\n",
    "```python\n",
    "class AnswerLengthEvaluator:\n",
    "    def __call__(self, response: str, **kwargs):\n",
    "        return {\"answer_length\": len(response)}\n",
    "```\n",
    "\n",
    "You can then integrate it with the local or cloud evaluation workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde67f1f",
   "metadata": {
    "id": "5.4-Simulators"
   },
   "source": [
    "## 5.4 Simulators & Adversarial Data\n",
    "If you need to generate synthetic or adversarial evaluation data, the `azure-ai-evaluation` package provides simulators. \n",
    "\n",
    "For example, you can simulate adversarial queries using `AdversarialSimulator` to test model safety and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63eee3",
   "metadata": {
    "id": "6-Conclusion"
   },
   "source": [
    "# 6. Conclusion ðŸ\n",
    "\n",
    "We covered:\n",
    "1. **Local** evaluations with `evaluate(...)` on JSONL data (now including a groundedness metric).\n",
    "2. **Cloud** evaluations with `AIProjectClient` including retry logic for robustness.\n",
    "3. Built-in **risk & safety** and **quality** evaluators.\n",
    "4. **Custom** evaluators for advanced scenarios.\n",
    "5. **Simulators** for generating adversarial data.\n",
    "\n",
    "**Next Steps**:\n",
    "- Adjust your model and prompts based on evaluation feedback.\n",
    "- Integrate these evaluations into your CI/CD pipelines.\n",
    "- Combine with observability tools for deeper insights.\n",
    "\n",
    "> **Best of luck** building robust and responsible AI solutions with Azure AI Foundry!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
