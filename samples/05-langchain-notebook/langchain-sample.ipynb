{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043c1d5c",
   "metadata": {},
   "source": [
    "# Sample 5: LangChain with AI Foundry\n",
    "\n",
    "This notebook demonstrates using LangChain to interact with AI Foundry models. LangChain provides a higher-level abstraction over the OpenAI API with additional features like prompt templates, chains, and memory.\n",
    "\n",
    "## Key Features\n",
    "- LangChain ChatOpenAI integration\n",
    "- Prompt templates\n",
    "- Conversation memory\n",
    "- Chain operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704f126",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cbb99b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AZURE_OPENAI_ENDPOINT loaded\n",
      "âœ… AZURE_OPENAI_API_KEY loaded\n",
      "âœ… AZURE_OPENAI_DEPLOYMENT_NAME loaded\n",
      "\n",
      "ðŸ”— Endpoint: https://chwestbr-foundry-projec-resource.openai.azure.com/\n",
      "ðŸ¤– Model: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('../../.env')\n",
    "\n",
    "# Verify required environment variables\n",
    "required_vars = [\n",
    "    'AZURE_OPENAI_ENDPOINT',\n",
    "    'AZURE_OPENAI_API_KEY', \n",
    "    'AZURE_OPENAI_DEPLOYMENT_NAME'\n",
    "]\n",
    "\n",
    "for var in required_vars:\n",
    "    if not os.getenv(var):\n",
    "        print(f\"âŒ Missing {var}\")\n",
    "    else:\n",
    "        print(f\"âœ… {var} loaded\")\n",
    "        \n",
    "print(f\"\\nðŸ”— Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"ðŸ¤– Model: {os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f8217",
   "metadata": {},
   "source": [
    "## 2. LangChain Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f611aa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦œ LangChain ChatOpenAI initialized (using chat completion API)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# Initialize Azure OpenAI through LangChain\n",
    "# Note: AzureChatOpenAI uses the chat completion API (not completion API)\n",
    "# This ensures we get proper conversation context and system message support\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    api_key=os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "    api_version=os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview'),\n",
    "    deployment_name=os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME'),\n",
    "    temperature=0.7,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(\"ðŸ¦œ LangChain ChatOpenAI initialized (using chat completion API)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec0e2b",
   "metadata": {},
   "source": [
    "## 3. Basic Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c45206b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– AI Response:\n",
      "Of course! Here's one for you:\n",
      "\n",
      "Why don't skeletons fight each other?\n",
      "\n",
      "Because they don't have the guts! ðŸ˜„\n"
     ]
    }
   ],
   "source": [
    "# Simple message invocation\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Hello! Can you tell me a joke?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"ðŸ¤– AI Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c210ba2",
   "metadata": {},
   "source": [
    "## 4. Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4075b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Template Response:\n",
      "Learning Python is an exciting journey, and you're going to have a lot of fun with it! Python is beginner-friendly and very versatile, so you're already on the right track by choosing it. Hereâ€™s a roadmap to help you get started and stay motivated:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Start with the Basics**\n",
      "- **Install Python**: Download and install the latest version of Python from [python.org](https://www.python.org/).\n",
      "- **Learn Syntax and Fundamentals**: Start with basic concepts like variables, data types, loops, conditionals, and functions. There are many free resources like [w3schools](https://www.w3schools.com/python/) and [Pythonâ€™s official tutorial](https://docs.python.org/3/tutorial/).\n",
      "- **Interactive Platforms**: Use beginner-friendly platforms like [Codecademy](https://www.codecademy.com/learn/learn-python-3), [freeCodeCamp](https://www.freecodecamp.org/), or [Real Python](https://realpython.com/).\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Practice, Practice, Practice**\n",
      "- **Write Code Daily**: Set aside some time each day to practice. Even 30 minutes a day can make a big difference.\n",
      "- **Use an Interactive Shell**: Play around with Pythonâ€™s REPL (Read-Eval-Print Loop) to test small pieces of code quickly.\n",
      "- **Build Small Projects**: Start with simple projects, like a calculator, a to-do list app, or a number guessing game.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Leverage Online Resources**\n",
      "- **YouTube Tutorials**: Channels like [Programming with Mosh](https://www.youtube.com/c/programmingwithmosh) and [Corey Schafer](https://www.youtube.com/c/Coreyms) offer excellent Python tutorials.\n",
      "- **Books**: Consider beginner-friendly books like *\"Automate the Boring Stuff with Python\"* by Al Sweigart or *\"Python Crash Course\"* by Eric Matthes.\n",
      "- **Documentation**: Donâ€™t hesitate to explore Pythonâ€™s official documentationâ€”itâ€™s well-written and thorough.\n",
      "\n",
      "---\n",
      "\n",
      "### **4. Work on Projects**\n",
      "- As you gain confidence, challenge yourself with real-world projects to solidify your learning:\n",
      "  - **Beginner**: Todo app, basic web scraper, or text-based adventure game.\n",
      "  - **Intermediate**: Weather app, budget tracker, or blog website using Flask/Django.\n",
      "  - **Advanced**: Machine learning model,\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert {role}. Provide helpful advice in a {tone} tone.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "# Use the template\n",
    "prompt = prompt_template.format_messages(\n",
    "    role=\"software developer\",\n",
    "    tone=\"friendly and encouraging\", \n",
    "    question=\"What's the best way to learn Python?\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(\"ðŸŽ¯ Template Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1bf9a",
   "metadata": {},
   "source": [
    "## 5. Conversation Chain with Memory\n",
    "\n",
    "> **ðŸ“ Note on Jupyter Output:** Due to a known Jupyter quirk with async operations, this example is split across multiple cells to avoid output duplication. The pattern is: Setup â†’ Execute â†’ Display. This separation also makes the code flow clearer by separating concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a2bb1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’­ Conversation setup complete! Ready to chat with memory...\n"
     ]
    }
   ],
   "source": [
    "# Create conversation with automatic memory - the modern LangChain way!\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create a simple prompt template that includes message history\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the basic chain\n",
    "chain = prompt | llm\n",
    "\n",
    "# Create message history store (in memory for this demo)\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Wrap the chain with message history - LangChain handles everything automatically!\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "print(\"ðŸ’­ Conversation setup complete! Ready to chat with memory...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "092ce3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo conversation with automatic memory\n",
    "session_id = \"demo_session\"\n",
    "\n",
    "# First exchange - just chat naturally\n",
    "response1 = conversation.invoke(\n",
    "    {\"input\": \"My name is Alice and I love Python programming.\"}, \n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "\n",
    "# Second exchange - LangChain automatically remembers previous messages\n",
    "response2 = conversation.invoke(\n",
    "    {\"input\": \"What's my name and what do I love?\"}, \n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58953698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—£ï¸ Conversation Results:\n",
      "\n",
      "ðŸ‘¤ User: My name is Alice and I love Python programming.\n",
      "ðŸ¤– AI: Hi, Alice! It's great to meet you. Python programming is awesomeâ€”whether you're building websites, analyzing data, or automating tasks, Python can do it all. What kinds of projects are you working on, or what do you enjoy most about Python?\n",
      "\n",
      "ðŸ‘¤ User: What's my name and what do I love?\n",
      "ðŸ¤– AI: Your name is **Alice**, and you love **Python programming**! ðŸ˜Š\n",
      "\n",
      "ðŸ“š LangChain automatically stored 4 messages!\n",
      "ðŸ‘¤ User: My name is Alice and I love Python programming.\n",
      "ðŸ¤– AI: Hi, Alice! It's great to meet you. Python programming is awe...\n",
      "ðŸ‘¤ User: What's my name and what do I love?\n",
      "ðŸ¤– AI: Your name is **Alice**, and you love **Python programming**!...\n"
     ]
    }
   ],
   "source": [
    "# Display the conversation results\n",
    "print(\"ðŸ—£ï¸ Conversation Results:\\n\")\n",
    "\n",
    "print(\"ðŸ‘¤ User: My name is Alice and I love Python programming.\")\n",
    "print(f\"ðŸ¤– AI: {response1.content}\\n\")\n",
    "\n",
    "print(\"ðŸ‘¤ User: What's my name and what do I love?\")\n",
    "print(f\"ðŸ¤– AI: {response2.content}\\n\")\n",
    "\n",
    "# Show the conversation history that LangChain automatically maintained\n",
    "history = get_session_history(session_id)\n",
    "print(f\"ðŸ“š LangChain automatically stored {len(history.messages)} messages!\")\n",
    "for msg in history.messages:\n",
    "    msg_type = \"ðŸ‘¤ User\" if msg.type == \"human\" else \"ðŸ¤– AI\"\n",
    "    print(f\"{msg_type}: {msg.content[:60]}{'...' if len(msg.content) > 60 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca26d0",
   "metadata": {},
   "source": [
    "## 6. Chaining: Simple and Advanced Examples\n",
    "\n",
    "This section showcases LangChain's core strength - **chaining operations** to create multi-step workflows. We'll start with simple chaining and then show a more complex content creation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76ddfd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”— Simple chains created!\n",
      "   â€¢ Analyze chain: Takes topic â†’ Returns insights\n",
      "   â€¢ Summarize chain: Takes insights â†’ Returns summary\n",
      "\n",
      "âš¡ Next: We'll chain them together!\n"
     ]
    }
   ],
   "source": [
    "# SIMPLE CHAINING EXAMPLE\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a simple two-step chain: Analyze â†’ Summarize\n",
    "analyze_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an analyst. Analyze the given topic and provide 3 key insights.\"),\n",
    "    (\"human\", \"Topic: {topic}\")\n",
    "])\n",
    "\n",
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a writer. Take these insights and write a brief, engaging summary.\"),\n",
    "    (\"human\", \"Insights: {insights}\")\n",
    "])\n",
    "\n",
    "# Build individual chains\n",
    "analyze_chain = analyze_prompt | llm | StrOutputParser()\n",
    "summarize_chain = summarize_prompt | llm | StrOutputParser()\n",
    "\n",
    "print(\"ðŸ”— Simple chains created!\")\n",
    "print(\"   â€¢ Analyze chain: Takes topic â†’ Returns insights\")  \n",
    "print(\"   â€¢ Summarize chain: Takes insights â†’ Returns summary\")\n",
    "print(\"\\nâš¡ Next: We'll chain them together!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "605d78d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating simple chain: Analyze -> Summarize\n",
      "Executing simple chain for 'remote work productivity'...\n",
      "This will: Analyze topic -> Generate summary\n",
      "\n",
      "Simple chain complete!\n",
      "Final summary: Remote work offers employees the flexibility to tailor their schedules and environments for maximum ...\n",
      "\n",
      "This demonstrates LangChain's core value:\n",
      "   â€¢ Automatic data flow between steps\n",
      "   â€¢ Single invoke() call for multi-step workflow\n",
      "   â€¢ Clean, declarative pipeline definition\n",
      "Simple chain complete!\n",
      "Final summary: Remote work offers employees the flexibility to tailor their schedules and environments for maximum ...\n",
      "\n",
      "This demonstrates LangChain's core value:\n",
      "   â€¢ Automatic data flow between steps\n",
      "   â€¢ Single invoke() call for multi-step workflow\n",
      "   â€¢ Clean, declarative pipeline definition\n"
     ]
    }
   ],
   "source": [
    "# SIMPLE CHAINING - No data transformation needed!\n",
    "print(\"Creating simple chain: Analyze -> Summarize\")\n",
    "\n",
    "# The problem: Our chains expect different input keys\n",
    "# analyze_chain expects: {\"topic\": \"...\"}\n",
    "# summarize_chain expects: {\"insights\": \"...\"}\n",
    "\n",
    "# LangChain solution: Use RunnablePassthrough to map outputs to inputs\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create a simple chain that pipes the output of one to the input of the next\n",
    "simple_chain = (\n",
    "    analyze_chain\n",
    "    | (lambda insights: {\"insights\": insights})  # Just map the string to the expected key\n",
    "    | summarize_chain\n",
    ")\n",
    "\n",
    "# Execute the simple chain\n",
    "topic = \"remote work productivity\"\n",
    "\n",
    "print(f\"Executing simple chain for '{topic}'...\")\n",
    "print(\"This will: Analyze topic -> Generate summary\\n\")\n",
    "\n",
    "result = simple_chain.invoke({\"topic\": topic})\n",
    "\n",
    "print(\"Simple chain complete!\")\n",
    "print(f\"Final summary: {result[:100]}...\")\n",
    "\n",
    "print(\"\\nThis demonstrates LangChain's core value:\")\n",
    "print(\"   â€¢ Automatic data flow between steps\")\n",
    "print(\"   â€¢ Single invoke() call for multi-step workflow\")\n",
    "print(\"   â€¢ Clean, declarative pipeline definition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "762746fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ðŸš€ ADVANCED EXAMPLE: Content Creation Pipeline\n",
      "============================================================\n",
      "ðŸ“ Content pipeline created: Research â†’ Draft â†’ Review\n",
      "\n",
      "ðŸŽ¯ Executing content pipeline for 'benefits of remote work'...\n",
      "âš¡ Single call will: Research â†’ Write Draft â†’ Generate Review\n",
      "\n",
      "âœ… Content pipeline complete!\n",
      "ðŸ“‹ Editor review: This draft effectively highlights the benefits of remote work, but there are areas where clarity, depth, and engagement ...\n",
      "\n",
      "ðŸ’¡ Advanced pipeline benefits:\n",
      "   â€¢ 3-step workflow in one call\n",
      "   â€¢ Automatic context passing\n",
      "   â€¢ Complex content creation made simple\n",
      "   â€¢ Each step builds on the previous\n",
      "âœ… Content pipeline complete!\n",
      "ðŸ“‹ Editor review: This draft effectively highlights the benefits of remote work, but there are areas where clarity, depth, and engagement ...\n",
      "\n",
      "ðŸ’¡ Advanced pipeline benefits:\n",
      "   â€¢ 3-step workflow in one call\n",
      "   â€¢ Automatic context passing\n",
      "   â€¢ Complex content creation made simple\n",
      "   â€¢ Each step builds on the previous\n"
     ]
    }
   ],
   "source": [
    "# ADVANCED CHAINING - Content Creation Pipeline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸš€ ADVANCED EXAMPLE: Content Creation Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# More sophisticated chains for content creation\n",
    "research_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a research expert. Generate 5 bullet points about the topic.\"),\n",
    "    (\"human\", \"Research topic: {topic}\")\n",
    "])\n",
    "\n",
    "draft_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a content writer. Write a short blog post based on these research points.\"),\n",
    "    (\"human\", \"Research: {research}\")\n",
    "])\n",
    "\n",
    "review_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an editor. Review this draft and suggest 3 improvements.\"),\n",
    "    (\"human\", \"Draft to review: {draft}\")\n",
    "])\n",
    "\n",
    "# Build the chains\n",
    "research_chain = research_prompt | llm | StrOutputParser()\n",
    "draft_chain = draft_prompt | llm | StrOutputParser()  \n",
    "review_chain = review_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Create the full content pipeline: Research â†’ Draft â†’ Review\n",
    "content_pipeline = (\n",
    "    research_chain\n",
    "    | (lambda research: {\"research\": research})\n",
    "    | draft_chain\n",
    "    | (lambda draft: {\"draft\": draft})\n",
    "    | review_chain\n",
    ")\n",
    "\n",
    "print(\"ðŸ“ Content pipeline created: Research â†’ Draft â†’ Review\")\n",
    "\n",
    "# Execute the advanced pipeline\n",
    "topic = \"benefits of remote work\"\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Executing content pipeline for '{topic}'...\")\n",
    "print(\"âš¡ Single call will: Research â†’ Write Draft â†’ Generate Review\\n\")\n",
    "\n",
    "result = content_pipeline.invoke({\"topic\": topic})\n",
    "\n",
    "print(\"âœ… Content pipeline complete!\")\n",
    "print(f\"ðŸ“‹ Editor review: {result[:120]}...\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Advanced pipeline benefits:\")\n",
    "print(\"   â€¢ 3-step workflow in one call\")\n",
    "print(\"   â€¢ Automatic context passing\")\n",
    "print(\"   â€¢ Complex content creation made simple\")\n",
    "print(\"   â€¢ Each step builds on the previous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ed49a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸ”— LANGCHAIN CHAINING vs MANUAL ORCHESTRATION\n",
      "======================================================================\n",
      "\n",
      "âŒ MANUAL APPROACH (what we avoided):\n",
      "   # Research step\n",
      "   research_result = research_chain.invoke({'topic': topic})\n",
      "   # Draft step\n",
      "   draft_result = draft_chain.invoke({'research': research_result})\n",
      "   # Review step\n",
      "   final_result = review_chain.invoke({'draft': draft_result})\n",
      "   # Multiple calls, manual data passing, error handling\n",
      "\n",
      "âœ… LANGCHAIN CHAINING:\n",
      "   # Single call does everything!\n",
      "   result = content_pipeline.invoke({'topic': topic})\n",
      "   # Automatic data flow, built-in error handling\n",
      "\n",
      "ðŸ“Š PIPELINE RESULT:\n",
      "--------------------------------------------------\n",
      "This draft effectively highlights the benefits of remote work, but there are areas where clarity, depth, and engagement can be improved. Here are three suggested improvements:\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **Enhance t...\n",
      "\n",
      "ðŸŽ¯ KEY BENEFITS OF CHAINING:\n",
      "   â€¢ Single invoke() call for multi-step workflows\n",
      "   â€¢ Automatic data flow between pipeline steps\n",
      "   â€¢ Built-in error handling and retries\n",
      "   â€¢ Clean, readable pipeline definitions\n",
      "   â€¢ Reusable and composable components\n",
      "\n",
      "ðŸ’¡ COMPARE TO RAW OPENAI SDK:\n",
      "   â€¢ 3 separate API calls to manage\n",
      "   â€¢ Manual data transformation between steps\n",
      "   â€¢ Custom error handling for each step\n",
      "   â€¢ Context passing and state management\n",
      "   â€¢ Retry logic implementation\n",
      "\n",
      "âœ¨ SIMPLE YET POWERFUL:\n",
      "   â€¢ Basic chaining: analyze_chain | summarize_chain\n",
      "   â€¢ Advanced pipeline: research â†’ draft â†’ review\n",
      "   â€¢ Same pattern scales from 2 steps to 20+ steps\n",
      "\n",
      "======================================================================\n",
      "ðŸš€ This is why LangChain excels at AI workflows!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Show what chaining accomplishes vs manual orchestration\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ”— LANGCHAIN CHAINING vs MANUAL ORCHESTRATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nâŒ MANUAL APPROACH (what we avoided):\")\n",
    "print(\"   # Research step\")\n",
    "print(\"   research_result = research_chain.invoke({'topic': topic})\")\n",
    "print(\"   # Draft step\") \n",
    "print(\"   draft_result = draft_chain.invoke({'research': research_result})\")\n",
    "print(\"   # Review step\")\n",
    "print(\"   final_result = review_chain.invoke({'draft': draft_result})\")\n",
    "print(\"   # Multiple calls, manual data passing, error handling\")\n",
    "\n",
    "print(\"\\nâœ… LANGCHAIN CHAINING:\")\n",
    "print(\"   # Single call does everything!\")\n",
    "print(\"   result = content_pipeline.invoke({'topic': topic})\")\n",
    "print(\"   # Automatic data flow, built-in error handling\")\n",
    "\n",
    "print(f\"\\nðŸ“Š PIPELINE RESULT:\")\n",
    "print(\"-\" * 50)\n",
    "print(result[:200] + \"...\" if len(result) > 200 else result)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ KEY BENEFITS OF CHAINING:\")\n",
    "print(\"   â€¢ Single invoke() call for multi-step workflows\")\n",
    "print(\"   â€¢ Automatic data flow between pipeline steps\") \n",
    "print(\"   â€¢ Built-in error handling and retries\")\n",
    "print(\"   â€¢ Clean, readable pipeline definitions\")\n",
    "print(\"   â€¢ Reusable and composable components\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ COMPARE TO RAW OPENAI SDK:\")\n",
    "print(\"   â€¢ 3 separate API calls to manage\")\n",
    "print(\"   â€¢ Manual data transformation between steps\")  \n",
    "print(\"   â€¢ Custom error handling for each step\")\n",
    "print(\"   â€¢ Context passing and state management\")\n",
    "print(\"   â€¢ Retry logic implementation\")\n",
    "\n",
    "print(f\"\\nâœ¨ SIMPLE YET POWERFUL:\")\n",
    "print(\"   â€¢ Basic chaining: analyze_chain | summarize_chain\")\n",
    "print(\"   â€¢ Advanced pipeline: research â†’ draft â†’ review\")\n",
    "print(\"   â€¢ Same pattern scales from 2 steps to 20+ steps\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸš€ This is why LangChain excels at AI workflows!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a90274e",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "This notebook demonstrated LangChain's core concepts and advanced capabilities:\n",
    "\n",
    "âœ… **LangChain Setup**: Initialized AzureChatOpenAI with environment configuration (uses chat completion API)\n",
    "\n",
    "âœ… **Basic Chat**: Simple message invocation with system and human messages\n",
    "\n",
    "âœ… **Prompt Templates**: Reusable prompt structures with variable substitution\n",
    "\n",
    "âœ… **Modern Memory**: Automatic conversation management using `RunnableWithMessageHistory`\n",
    "\n",
    "âœ… **Advanced Chaining**: Multi-step content creation pipeline (Research â†’ Draft â†’ Review â†’ Polish)\n",
    "\n",
    "### Key LangChain Benefits:\n",
    "- **Chat Completion API**: Uses modern chat completion endpoints (not legacy completion)\n",
    "- **Abstraction**: Higher-level interface than raw OpenAI SDK\n",
    "- **Automatic Memory**: Built-in conversation history with `RunnableWithMessageHistory`\n",
    "- **Templates**: Reusable prompt structures with variable substitution\n",
    "- **Chain Orchestration**: Seamlessly coordinate multi-step AI workflows\n",
    "- **Session Management**: Easy conversation sessions with automatic message tracking\n",
    "\n",
    "### Why LangChain Shines:\n",
    "The **content creation pipeline** shows LangChain's real value - what would require complex manual orchestration (managing 4 separate API calls, passing context between steps, handling errors) becomes a simple, declarative workflow. Try building that same pipeline with raw OpenAI SDK calls!\n",
    "\n",
    "### Modern LangChain Patterns:\n",
    "- **RunnableWithMessageHistory**: Current standard for conversation memory\n",
    "- **MessagesPlaceholder**: Clean way to inject conversation history\n",
    "- **Chain Composition**: Use pipe operators for readable multi-step workflows\n",
    "- **Automatic Context Passing**: Data flows seamlessly between pipeline steps\n",
    "- **Error Handling**: Built-in resilience for complex workflows\n",
    "\n",
    "### Next Steps:\n",
    "- Explore LangChain agents for tool-calling workflows\n",
    "- Try different memory types and stores (Redis, SQL, etc.)\n",
    "- Experiment with conditional chains and branching logic\n",
    "- Integrate with vector databases for RAG applications\n",
    "\n",
    "This notebook focused on essential LangChain patterns, from basic chat to advanced multi-step orchestration - showing why LangChain excels at complex AI workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
